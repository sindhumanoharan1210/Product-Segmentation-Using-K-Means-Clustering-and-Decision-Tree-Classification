{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c807f0d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "warnings.filterwarnings(\"ignore\", message=\"KMeans is known to have a memory leak\")\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"Ramcocements.csv\")\n",
    "\n",
    "# View first 5 rows\n",
    "print(df.head())\n",
    "\n",
    "# Check structure\n",
    "print(df.info())\n",
    "df.columns = df.columns.str.strip()\n",
    "# Check missing values\n",
    "print(df.isnull().sum())\n",
    "features = [\n",
    "    \"Sales_Revenue\",\n",
    "    \"Units_Sold\",\n",
    "    \"Profit_Margin_Percent\",\n",
    "    \"Discount_Percent\",\n",
    "    \"Return_Rate_Percent\",\n",
    "    \"Customer_Rating\"\n",
    "]\n",
    "\n",
    "X = df[features]\n",
    "#EDA\n",
    "df.shape\n",
    "df.columns\n",
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# KMeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "df[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "# Evaluation\n",
    "print(\"Silhouette Score:\", silhouette_score(X_scaled, df[\"Cluster\"]))\n",
    "# Visualization\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(\n",
    "    x=df[\"Units_Sold\"],\n",
    "    y=df[\"Sales_Revenue\"],\n",
    "    hue=df[\"Cluster\"],\n",
    "    palette=\"Set1\"\n",
    ")\n",
    "plt.show()\n",
    "#KDE Plot\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "n_cols = 2\n",
    "n_rows = math.ceil(len(numeric_cols) / n_cols)\n",
    "\n",
    "plt.figure(figsize=(12, 5 * n_rows))\n",
    "\n",
    "for i, col in enumerate(numeric_cols, 1):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    sns.kdeplot(data=df, x=col, fill=True)\n",
    "    plt.title(f\"KDE Plot - {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Density\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "numeric_df = df.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Heatmap - Numerical Features\")\n",
    "plt.show()\n",
    "#PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "principal_components = pca.fit_transform(X_scaled)\n",
    "\n",
    "pca_df = pd.DataFrame(\n",
    "    data=principal_components,\n",
    "    columns=[\"PCA1\", \"PCA2\"]\n",
    ")\n",
    "\n",
    "pca_df.head()\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "scatter = plt.scatter(\n",
    "    pca_df[\"PCA1\"],\n",
    "    pca_df[\"PCA2\"],\n",
    "    c=pca_df[\"PCA1\"],\n",
    "    cmap=\"viridis\",\n",
    "    s=70\n",
    ")\n",
    "\n",
    "plt.colorbar(scatter)\n",
    "plt.title(\"PCA Projection (Color by PCA1 Intensity)\")\n",
    "plt.show()\n",
    "intertia = []\n",
    "range_val = range(1,15)\n",
    "for i in range_val:\n",
    "    KMean = KMeans(n_clusters=i)\n",
    "    KMean.fit_predict(pd.DataFrame(X_scaled))\n",
    "    intertia.append(KMean.inertia_)\n",
    "plt.plot(range_val,intertia,'bx-')\n",
    "plt.xlabel('Values of K')  # Corrected from xlable to xlabel\n",
    "plt.ylabel('Intertia')     # Corrected from ylable to ylabel\n",
    "plt.title('The Elbow Method using Intertia')\n",
    "plt.show()\n",
    "# Import necessary libraries\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming X_scaled and pca_df are already defined\n",
    "# Create and fit the KMeans model\n",
    "KMeans_model = KMeans(n_clusters=4)\n",
    "KMeans_model.fit(X_scaled)  \n",
    "# Add cluster labels to the dataframe\n",
    "pca_df_KMeans = pd.concat([pca_df, pd.DataFrame({'cluster': KMeans_model.labels_})], axis=1)\n",
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "# First add the Cluster column to the dataframe\n",
    "pca_df_KMeans[\"Cluster\"] = df[\"Cluster\"]\n",
    "\n",
    "# Then use the scatterplot with the Cluster column\n",
    "sns.scatterplot(\n",
    "    x=\"PCA1\",\n",
    "    y=\"PCA2\",\n",
    "    hue=\"Cluster\",          \n",
    "    data=pca_df_KMeans,     \n",
    "    palette=[\"red\", \"green\", \"blue\"], \n",
    "    s=80\n",
    ")\n",
    "\n",
    "plt.title(\"Clustering using K-Means Algorithm\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "n_clusters = 3\n",
    "\n",
    "# Train KMeans\n",
    "kmeans_model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "df[\"Cluster\"] = kmeans_model.fit_predict(X_scaled)\n",
    "# Get cluster centers (scaled)\n",
    "cluster_centers_scaled = kmeans_model.cluster_centers_\n",
    "\n",
    "# Inverse transform to original values\n",
    "cluster_centers_original = scaler.inverse_transform(cluster_centers_scaled)\n",
    "\n",
    "# Convert to DataFrame\n",
    "cluster_centers = pd.DataFrame(\n",
    "    data=cluster_centers_original,\n",
    "    columns=features\n",
    ")\n",
    "\n",
    "cluster_centers\n",
    "cluster_centers.index.name = \"Cluster\"\n",
    "cluster_centers\n",
    "cluster_df = pd.concat(\n",
    "    [df.reset_index(drop=True),\n",
    "     pd.DataFrame({\"Cluster\": kmeans_model.labels_})],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "cluster_df.head()\n",
    "print(type(cluster_df))\n",
    "print(cluster_df.shape)\n",
    "print(cluster_df.head())\n",
    "# Add cluster column directly (Best Practice)\n",
    "df[\"Cluster\"] = kmeans_model.labels_\n",
    "\n",
    "cluster_df = df.copy()\n",
    "numeric_cols = [\n",
    "    \"Sales_Revenue\",\n",
    "    \"Units_Sold\",\n",
    "    \"Profit_Margin_Percent\",\n",
    "    \"Discount_Percent\",\n",
    "    \"Return_Rate_Percent\",\n",
    "    \"Customer_Rating\"\n",
    "]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    \n",
    "    g = sns.FacetGrid(cluster_df, col=\"Cluster\", height=4)\n",
    "    g.map_dataframe(sns.histplot, x=col, bins=20)\n",
    "    \n",
    "    g.fig.suptitle(f\"Distribution of {col} by Cluster\", y=1.02)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = cluster_df[features]\n",
    "y = cluster_df[\"Cluster\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "X_train\n",
    "# ------------------------------\n",
    "# Decision Tree Model\n",
    "# ------------------------------\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Train model using entropy\n",
    "dt_model = DecisionTreeClassifier(criterion=\"entropy\", random_state=42)\n",
    "\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_pred = dt_model.predict(X_test)\n",
    "# ------------------------------\n",
    "# Confusion Matrix\n",
    "# ------------------------------\n",
    "\n",
    "print(\"Confusion Matrix:\\n\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))\n",
    "import joblib\n",
    "\n",
    "# Save model\n",
    "joblib.dump(dt_model, \"ramco_decision_tree.pkl\")\n",
    "\n",
    "# Load model\n",
    "loaded_model = joblib.load(\"ramco_decision_tree.pkl\")\n",
    "\n",
    "print(\"Accuracy:\", loaded_model.score(X_test, y_test) * 100, \"%\")\n",
    "# ----------------------------------\n",
    "# Saving All Trained Models & Data\n",
    "# ----------------------------------\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Save KMeans model\n",
    "joblib.dump(kmeans_model, \"ramco_kmeans_model.pkl\")\n",
    "\n",
    "# Save Scaler\n",
    "joblib.dump(scaler, \"ramco_scaler.pkl\")\n",
    "\n",
    "# Save Decision Tree model\n",
    "joblib.dump(dt_model, \"ramco_decision_tree_model.pkl\")\n",
    "\n",
    "# Save Clustered Dataset\n",
    "cluster_df.to_csv(\"Ramco_Product_Segmentation_Data.csv\", index=False)\n",
    "\n",
    "print(\"All models and clustered dataset saved successfully.\")\n",
    "joblib.dump(kmeans_model, \"ramco_kmeans_v1.pkl\")\n",
    "joblib.dump(scaler, \"ramco_scaler_v1.pkl\")\n",
    "joblib.dump(dt_model, \"ramco_decision_tree_v1.pkl\")\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Create folder if it doesn't exist\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Save models inside folder\n",
    "joblib.dump(kmeans_model, \"models/ramco_kmeans_v1.pkl\")\n",
    "joblib.dump(scaler, \"models/ramco_scaler_v1.pkl\")\n",
    "joblib.dump(dt_model, \"models/ramco_decision_tree_v1.pkl\")\n",
    "\n",
    "print(\"Models saved successfully in 'models' folder.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
